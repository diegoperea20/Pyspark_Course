{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1KNVgggmQxe7hCoLHk9rzIjRgPtACqnbS","authorship_tag":"ABX9TyO5w0beK+m5Qj/8wmgxJb2n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-gKwJxLzF7C","executionInfo":{"status":"ok","timestamp":1676661345104,"user_tz":300,"elapsed":49199,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"d1aa47a5-603e-46fc-c442-6bd341ed7e55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=c9bbeb5820fd96d685e4463b66fd1caaae845d0521830de95232e6e020d70658\n","  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["import pyspark"],"metadata":{"id":"8wd1wCmNzRMp","executionInfo":{"status":"ok","timestamp":1676661682736,"user_tz":300,"elapsed":237,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark=SparkSession.builder.appName('Practise').getOrCreate()"],"metadata":{"id":"OIGi9KtBzafT","executionInfo":{"status":"ok","timestamp":1676661692199,"user_tz":300,"elapsed":8262,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["##read the dataset\n","df_pyspark=spark.read.format(\"csv\").options(delimiter=\";\", header=True ).load(\"/content/drive/MyDrive/Pypask_Course/test4Columns.csv\")"],"metadata":{"id":"Gjkk2KGv0qcN","executionInfo":{"status":"ok","timestamp":1676663613289,"user_tz":300,"elapsed":1278,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["df_pyspark.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUekPWS908Lc","executionInfo":{"status":"ok","timestamp":1676661751639,"user_tz":300,"elapsed":1309,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"1c211e68-d88c-4695-c772-dffe93ee73b1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----+----------+------+\n","| name| age|experience|salary|\n","+-----+----+----------+------+\n","|name1|  30|         1|  3000|\n","|name2|  30|         2|   201|\n","|name3|  31|         3|   202|\n","|name4|  31|         3|   203|\n","|name5|  31|         3|   203|\n","|name6|  31|         3|   203|\n","|name7|null|      null|   203|\n","| null|  31|         3|   203|\n","| null|  31|      null|  null|\n","+-----+----+----------+------+\n","\n"]}]},{"cell_type":"markdown","source":["####Eliminar columna y valores nulos de las columnas"],"metadata":{"id":"KbJWj1ZM1FBo"}},{"cell_type":"code","source":["df_pyspark.drop('name').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1C1_dCiu1Dt8","executionInfo":{"status":"ok","timestamp":1676661993575,"user_tz":300,"elapsed":1478,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"c4d903d0-aa30-4f61-ff94-74c82f4b7e30"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----------+------+\n","| age|experience|salary|\n","+----+----------+------+\n","|  30|         1|  3000|\n","|  30|         2|   201|\n","|  31|         3|   202|\n","|  31|         3|   203|\n","|  31|         3|   203|\n","|  31|         3|   203|\n","|null|      null|   203|\n","|  31|         3|   203|\n","|  31|      null|  null|\n","+----+----------+------+\n","\n"]}]},{"cell_type":"code","source":["df_pyspark.na.drop().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3kQDhq02BMQ","executionInfo":{"status":"ok","timestamp":1676662063205,"user_tz":300,"elapsed":863,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"7513d033-0a8a-402d-c78b-2ad232e90cd6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+----------+------+\n","| name|age|experience|salary|\n","+-----+---+----------+------+\n","|name1| 30|         1|  3000|\n","|name2| 30|         2|   201|\n","|name3| 31|         3|   202|\n","|name4| 31|         3|   203|\n","|name5| 31|         3|   203|\n","|name6| 31|         3|   203|\n","+-----+---+----------+------+\n","\n"]}]},{"cell_type":"markdown","source":["na.drop(how=\"any\") es utilizado en PySpark para eliminar todas las filas que contienen valores nulos o faltantes en un DataFrame."],"metadata":{"id":"YcxaG54u35bi"}},{"cell_type":"code","source":["#df_pyspark.na.drop(how=\"any\", thresh=2) elimina todas las filas que contienen al menos un valor nulo (es decir, \"NaN\")\n"," #en el DataFrame df_pyspark, pero solo después de haber verificado que al menos dos valores no nulos se encuentran\n","  #en cada fila.\n","df_pyspark.na.drop(how=\"any\" , thresh=2).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIPttVNG3CMG","executionInfo":{"status":"ok","timestamp":1676662380768,"user_tz":300,"elapsed":860,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"79d74dcb-8333-4297-eac5-d5cb589a716d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----+----------+------+\n","| name| age|experience|salary|\n","+-----+----+----------+------+\n","|name1|  30|         1|  3000|\n","|name2|  30|         2|   201|\n","|name3|  31|         3|   202|\n","|name4|  31|         3|   203|\n","|name5|  31|         3|   203|\n","|name6|  31|         3|   203|\n","|name7|null|      null|   203|\n","| null|  31|         3|   203|\n","+-----+----+----------+------+\n","\n"]}]},{"cell_type":"code","source":["#df_pyspark.na.drop(how=\"any\" , subset=['experience']).show() elimina datos nulos de una columan especifica\n","df_pyspark.na.drop(how=\"any\" , subset=['experience']).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7yxkTJP37T-","executionInfo":{"status":"ok","timestamp":1676662932905,"user_tz":300,"elapsed":630,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"7877074b-5a2d-426f-ca0d-251fff7dfbbb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+----------+------+\n","| name|age|experience|salary|\n","+-----+---+----------+------+\n","|name1| 30|         1|  3000|\n","|name2| 30|         2|   201|\n","|name3| 31|         3|   202|\n","|name4| 31|         3|   203|\n","|name5| 31|         3|   203|\n","|name6| 31|         3|   203|\n","| null| 31|         3|   203|\n","+-----+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["#llenando valores nulos\n","#df_pyspark.na.fill('mising values :)').show() #Para todas las columnas\n","#df_pyspark.na.fill('mising values :)',['experience','name']).show() \n","df_pyspark.na.fill('mising values :)','experience').show() #para colum especifica"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lqukfgJ45rMM","executionInfo":{"status":"ok","timestamp":1676663136893,"user_tz":300,"elapsed":860,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"8f074166-2de0-4f19-fdae-1810fab9dd5c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----+----------------+------+\n","| name| age|      experience|salary|\n","+-----+----+----------------+------+\n","|name1|  30|               1|  3000|\n","|name2|  30|               2|   201|\n","|name3|  31|               3|   202|\n","|name4|  31|               3|   203|\n","|name5|  31|               3|   203|\n","|name6|  31|               3|   203|\n","|name7|null|mising values :)|   203|\n","| null|  31|               3|   203|\n","| null|  31|mising values :)|  null|\n","+-----+----+----------------+------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","df_pyspark = df_pyspark.withColumn(\"age\", col(\"age\").cast(\"integer\"))\n","df_pyspark = df_pyspark.withColumn(\"experience\", col(\"experience\").cast(\"integer\"))\n","df_pyspark = df_pyspark.withColumn(\"salary\", col(\"salary\").cast(\"integer\"))"],"metadata":{"id":"jjglHQhZ8DJV","executionInfo":{"status":"ok","timestamp":1676663692509,"user_tz":300,"elapsed":234,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\n","\n","imputer = Imputer(\n","    inputCols=['age', 'experience', 'salary'], \n","    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'experience', 'salary']]\n","    ).setStrategy(\"median\")"],"metadata":{"id":"DL5cgKJa6gGu","executionInfo":{"status":"ok","timestamp":1676663694325,"user_tz":300,"elapsed":217,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Add imputation cols to df\n","imputer.fit(df_pyspark).transform(df_pyspark).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WacaMtEX66_G","executionInfo":{"status":"ok","timestamp":1676663698043,"user_tz":300,"elapsed":2034,"user":{"displayName":"subenomai haku","userId":"01968901610098885489"}},"outputId":"901fc3a7-fd22-4965-86b2-18894cbdc1af"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----+----------+------+-----------+------------------+--------------+\n","| name| age|experience|salary|age_imputed|experience_imputed|salary_imputed|\n","+-----+----+----------+------+-----------+------------------+--------------+\n","|name1|  30|         1|  3000|         30|                 1|          3000|\n","|name2|  30|         2|   201|         30|                 2|           201|\n","|name3|  31|         3|   202|         31|                 3|           202|\n","|name4|  31|         3|   203|         31|                 3|           203|\n","|name5|  31|         3|   203|         31|                 3|           203|\n","|name6|  31|         3|   203|         31|                 3|           203|\n","|name7|null|      null|   203|         31|                 3|           203|\n","| null|  31|         3|   203|         31|                 3|           203|\n","| null|  31|      null|  null|         31|                 3|           203|\n","+-----+----+----------+------+-----------+------------------+--------------+\n","\n"]}]}]}